# Self-Synthesized Rehearsal for Continual Learning

## 1. Overview & Motivation

Self-Synthesized Rehearsal (SSR) is a memory-efficient continual learning method specifically designed for large language models (LLMs). Instead of storing real training examples from previous tasks (which requires substantial memory), SSR enables the model to generate synthetic examples that capture the essential knowledge from past tasks. These self-generated examples are then used for rehearsal when learning new tasks.

### Why Self-Synthesized Rehearsal?

**Key Innovation:**
SSR treats the LLM itself as a **generative replay mechanism**. The model learns to synthesize its own training data that encapsulates previous task knowledge, eliminating the need for external memory buffers while providing effective rehearsal.

**The Memory Problem in Continual Learning:**
```
Traditional Rehearsal:
Task 1: Store 1000 examples → 10 MB
Task 2: Store 1000 examples → 10 MB
Task 10: Store 1000 examples → 10 MB
Total: 100 MB of real data storage

Self-Synthesized Rehearsal:
Task 1-10: Generate on-demand → 0 MB storage
Only store: Task descriptions + model parameters
```

**Key Advantages:**
- **Zero data storage**: No need to retain training examples (privacy-preserving)
- **Infinite replay capacity**: Generate unlimited synthetic examples on demand
- **Privacy preservation**: No real user data stored, reducing privacy risks
- **Curriculum reshaping**: Intelligently schedule synthetic vs. real data mixing
- **Quality control**: Filter low-quality generations to maintain learning effectiveness
- **Task-aware generation**: Condition synthesis on task descriptions for targeted replay
- **Scalability**: Memory cost independent of number of tasks

**Improvements over Traditional Rehearsal:**
- No memory buffer required (vs. Experience Replay)
- Privacy-preserving (vs. storing real data)
- Unlimited synthetic examples (vs. fixed buffer size)
- LLM-specific design (vs. generic rehearsal methods)
- Dynamic curriculum (vs. uniform sampling)

### When to Use Self-Synthesized Rehearsal

**Ideal For:**
- Large language model continual learning
- Privacy-sensitive applications (healthcare, finance)
- Memory-constrained deployment environments
- Scenarios where data retention is prohibited (GDPR compliance)
- Long task sequences where buffer size becomes limiting
- Generative tasks (text generation, QA, summarization)
- Research on generative replay methods

**Consider Alternatives:**
- Use **traditional rehearsal (ER)** if memory is not a constraint and you need strongest anti-forgetting
- Use **EWC/regularization** if you can't generate quality synthetic data
- Use **prompt-based methods** for Vision Transformers (L2P, CODA-Prompt)
- Use **parameter isolation** if model capacity can grow
- Use **distillation-based methods** for student-teacher scenarios

**Requirements:**
- Base model must be a generative LLM (GPT, T5, etc.)
- Tasks must be describable in natural language
- Generation quality must be reasonable (not applicable to early-stage pre-training)
- Sufficient computational budget for synthesis

**Limitations to Consider:**
- Synthesis quality depends on model capability
- Generator drift: synthetic data may diverge from real distribution
- Computational overhead for generation
- May not work well for highly discriminative tasks
- Requires careful quality filtering
- Not suitable for vision-only tasks without modifications

## 2. Theoretical Background

### Generative Replay Framework

Self-Synthesized Rehearsal is grounded in the **generative replay** paradigm for continual learning. When learning a sequence of tasks T₁, T₂, ..., Tₙ, we want to maintain performance on all tasks without storing data.

**Standard Continual Learning Objective:**
```
min_θ Σₜ₌₁ⁿ E_{(x,y)~Dₜ}[L(f_θ(x), y)]
```

subject to: Cannot access D₁, D₂, ..., Dₜ₋₁ when learning task t.

**Generative Replay Solution:**
Replace real data with synthetic data generated by the model:
```
D̃ₜ = {(x̃, ỹ) | x̃ ~ p_θ(x|t), ỹ ~ p_θ(y|x̃,t)}
```

**Training Objective with Replay:**
```
min_θ E_{(x,y)~Dₜ}[L(f_θ(x), y)] + λ E_{(x̃,ỹ)~D̃₁:ₜ₋₁}[L(f_θ(x̃), ỹ)]
                ↑                                      ↑
            New task loss                    Rehearsal on synthetic data
```

### Self-Synthesis Mechanism

Unlike external generative models, SSR uses the **same model** for both task learning and data synthesis:

**Two Roles of the Model:**
1. **Student**: Learning new tasks and maintaining old task performance
2. **Teacher**: Generating synthetic examples that encapsulate previous task knowledge

**Mathematical Formulation:**
```
θₜ = argmin_θ [L_new(θ; Dₜ) + L_replay(θ; G_θₜ₋₁(T₁:ₜ₋₁))]
              ↑                        ↑
          Learn task t        Rehearse on self-generated data
```

where:
- `G_θₜ₋₁(T₁:ₜ₋₁)`: Generator function using previous model θₜ₋₁
- `T₁:ₜ₋₁`: Task descriptions for tasks 1 to t-1

**Key Insight:**
The model at time t-1 encodes knowledge of tasks 1 to t-1. By generating examples with θₜ₋₁ conditioned on task descriptions, we extract this knowledge into synthetic training data.

### Task-Conditioned Generation

SSR leverages task descriptions for controlled generation:

**Generation Process:**
```
Task description: "Classify sentiment of movie reviews"

Synthesis prompt:
"Generate a training example for sentiment classification.
Input: [GENERATE]
Label: [GENERATE]"

Model generates:
Input: "This film was absolutely captivating and brilliant!"
Label: "positive"
```

**Conditional Distribution:**
```
p_θ(x, y | task_desc) = p_θ(x | task_desc) · p_θ(y | x, task_desc)
```

**Benefits of Task Conditioning:**
- Targeted generation for specific tasks
- Control over replay distribution
- Enables task-incremental learning
- Allows balancing across tasks

### Quality Filtering Theory

Not all synthetic examples are helpful. Quality filtering ensures generated data is useful:

**Quality Metrics:**

**1. Perplexity (Fluency):**
```
PPL(x, y) = exp(-1/N Σᵢ log p_θ(token_i | context))
```

Low perplexity → fluent, natural examples
High perplexity → unlikely, potentially malformed examples

**2. Consistency (Self-Agreement):**
```
consistency(x, y) = p_θ(y | x)
```

High consistency → model agrees with synthetic label
Low consistency → model doesn't support generated label

**3. Diversity (Coverage):**
```
diversity(D̃) = E_{x,x'~D̃}[distance(h(x), h(x'))]
```

where h(x) is a representation of x.

**Filtering Criterion:**
```
Keep example (x̃, ỹ) if:
    PPL(x̃, ỹ) < threshold_ppl AND
    consistency(x̃, ỹ) > threshold_cons AND
    diversity contribution is positive
```

### Curriculum Scheduling Theory

SSR adaptively mixes synthetic and real data based on training progress.

**Curriculum Ratio Schedule:**
```
α(t) = α_initial + (α_final - α_initial) · min(t / T_warmup, 1)
```

where:
- α(t): Synthetic data ratio at step t
- α_initial: Initial synthetic ratio (e.g., 0.2)
- α_final: Final synthetic ratio (e.g., 0.8)
- T_warmup: Warmup steps

**Training Batch Composition:**
```
Batch = (1 - α) × real_samples + α × synthetic_samples
```

**Intuition:**
- Early training: Focus on real data (α small)
  - Model needs strong signal from ground truth
  - Synthetic data quality may be poor initially
- Later training: Increase synthetic rehearsal (α large)
  - Model has learned task, needs to maintain it
  - Synthetic quality improves as model improves

**Adaptive Scheduling:**
```
if validation_loss < threshold:
    α(t) = α(t) + Δα  # Increase synthetic ratio
else:
    α(t) = α(t) - Δα  # Decrease synthetic ratio
```

## 3. Mathematical Formulation

### Self-Synthesis Algorithm

**Input:**
- Model θₜ₋₁ after training on tasks 1 to t-1
- Task descriptions {desc₁, desc₂, ..., descₜ₋₁}
- Number of synthetic examples N per task
- Quality thresholds (ppl_thresh, cons_thresh)

**Synthesis Procedure:**
```
For each previous task k ∈ {1, ..., t-1}:

    Initialize synthetic dataset: D̃ₖ = ∅

    While |D̃ₖ| < N:

        1. Create synthesis prompt:
           prompt = f"Generate example for: {descₖ}"

        2. Generate input:
           x̃ ~ p_θₜ₋₁(· | prompt)

        3. Generate label:
           ỹ ~ p_θₜ₋₁(· | x̃, descₖ)

        4. Compute quality scores:
           ppl = perplexity(x̃, ỹ)
           cons = consistency(x̃, ỹ)

        5. Filter:
           if ppl < ppl_thresh and cons > cons_thresh:
               D̃ₖ = D̃ₖ ∪ {(x̃, ỹ)}
               Assign confidence: c = cons

Return: D̃ = ∪ₖ D̃ₖ
```

### Training with Self-Synthesized Rehearsal

**Training Loop for Task t:**

**Phase 1: Mixed Training**
```
Initialize curriculum scheduler: α = α_initial

For each epoch e = 1 to E:
    For each real batch B_real from Dₜ:

        # Create mixed batch
        n_synthetic = floor(batch_size × α)
        n_real = batch_size - n_synthetic

        B_synthetic = sample(D̃₁:ₜ₋₁, n_synthetic)
        B_mixed = B_real[:n_real] + B_synthetic

        # Forward pass
        logits = model(B_mixed)
        loss = CrossEntropy(logits, labels)

        # Backward and update
        loss.backward()
        optimizer.step()

        # Update curriculum
        α = update_schedule(α, step)
```

**Phase 2: Synthesis for Current Task**
```
After training on task t:

    # Generate synthetic data for task t
    D̃ₜ = synthesize_examples(
        model=θₜ,
        task_description=descₜ,
        num_examples=N
    )

    # Filter by quality
    D̃ₜ = quality_filter(D̃ₜ, thresholds)

    # Store for future rehearsal
    Store D̃ₜ for future tasks
```

### Quality Filtering Formulas

**Perplexity Computation:**
```
Given sequence s = [token₁, token₂, ..., tokenₙ]:

PPL(s) = exp(-1/N Σᵢ₌₁ⁿ log p_θ(tokenᵢ | token₁:ᵢ₋₁))

Simplification:
PPL(s) = exp(-1/N · log_likelihood(s))
```

**Consistency Score:**
```
For classification:
consistency(x, y) = softmax(f_θ(x))[y]

For generation:
consistency(x, y) = exp(1/|y| Σₜ∈y log p_θ(t | x, y₁:ₜ₋₁))
```

**Diversity Promotion:**
```
Objective: Maximize pairwise distance in synthetic set

diversity_penalty = -λ_div · Σᵢ,ⱼ similarity(x̃ᵢ, x̃ⱼ)

Where similarity can be:
- Cosine similarity in embedding space
- BLEU/ROUGE for text
- Jaccard similarity for token sets
```

### Loss Function Decomposition

**Total Loss:**
```
L_total = L_task + λ_replay · L_replay + λ_quality · L_quality

Where:
    L_task = E_{(x,y)~Dₜ}[CrossEntropy(f_θ(x), y)]

    L_replay = E_{(x̃,ỹ)~D̃₁:ₜ₋₁}[CrossEntropy(f_θ(x̃), ỹ)]

    L_quality = E_{x̃~D̃}[-log consistency(x̃, ỹ̃)]
```

**Weighted Replay:**
Use confidence scores from quality filtering:
```
L_replay = E_{(x̃,ỹ,c)~D̃₁:ₜ₋₁}[c · CrossEntropy(f_θ(x̃), ỹ)]
```

High-confidence examples get higher weight in loss.

### Curriculum Scheduling Functions

**Linear Warmup:**
```
α(step) = α_min + (α_max - α_min) · min(step / T_warmup, 1)
```

**Cosine Schedule:**
```
α(step) = α_min + (α_max - α_min) · (1 + cos(π · step / T_total)) / 2
```

**Exponential Growth:**
```
α(step) = α_max - (α_max - α_min) · exp(-λ · step / T_warmup)
```

**Performance-Based Adaptive:**
```
If val_loss_new < threshold:
    α = min(α + Δα, α_max)  # Increase synthetic ratio
If val_loss_old > threshold:
    α = max(α - Δα, α_min)  # Decrease synthetic ratio
```

## 4. High-Level Intuition

### The Core Idea

Imagine a student learning multiple subjects sequentially without being able to review previous materials:

**Traditional Approach (Rehearsal with Memory):**
- Learn Math → Keep math textbook
- Learn Physics → Keep physics textbook
- Learn Chemistry → Review math and physics textbooks + learn chemistry
- **Problem**: Accumulating textbooks (memory)

**Self-Synthesized Rehearsal Approach:**
- Learn Math → Remember math knowledge in mind
- Learn Physics → **Self-generate** math practice problems from memory + learn physics
- Learn Chemistry → **Self-generate** math and physics problems + learn chemistry
- **Advantage**: No textbooks to store (just knowledge in mind)

### Why Self-Generation Works

The key insight is that **a trained model encodes task knowledge in its parameters**:

```
Model after Task 1:
θ₁ = contains knowledge of Task 1

Generate from θ₁:
x̃, ỹ ~ p_θ₁(·) → Examples reflect Task 1 knowledge

Train with these examples:
Maintains Task 1 knowledge in new model θ₂
```

**Knowledge Distillation Perspective:**
Self-synthesis is a form of self-distillation:
- Teacher: Model θₜ₋₁ (previous version)
- Student: Model θₜ (current version)
- Knowledge transfer: Via generated examples

### Generation Quality vs. Rehearsal Effectiveness

Not all synthetic examples are equally useful:

**High-Quality Synthetic Example:**
```
Task: Sentiment classification
Generated: "The movie was phenomenal with stunning visuals" → Positive
Quality: Fluent, consistent, representative
Effect: ✓ Effective rehearsal
```

**Low-Quality Synthetic Example:**
```
Task: Sentiment classification
Generated: "Movie the was with stunning visuals great" → Positive
Quality: Disfluent, unnatural
Effect: ✗ Harmful noise
```

**Quality Filtering Ensures:**
- Only fluent examples used for training
- Model doesn't learn from its own mistakes
- Synthetic distribution stays close to real distribution

### Curriculum Reshaping Intuition

The mixing ratio changes over training:

**Early Training (Low α):**
```
Batch: [Real₁, Real₂, Real₃, Real₄, Synthetic₁]
       80% real data, 20% synthetic
```
- Focus on learning the new task
- Minimal interference from potentially noisy synthetic data
- Strong gradient signal from ground truth

**Late Training (High α):**
```
Batch: [Real₁, Synthetic₁, Synthetic₂, Synthetic₃, Synthetic₄]
       20% real data, 80% synthetic
```
- New task is learned, focus on retention
- More rehearsal to prevent forgetting
- Model-generated data is higher quality now

**Why This Works:**
- Early: Model needs ground truth to learn task
- Late: Model has learned task, needs rehearsal to maintain
- Adaptive: Matches training phase to data source

### The Self-Improvement Cycle

SSR creates a virtuous cycle:

```
Better Model → Better Synthesis → Better Rehearsal → Better Model
    ↑                                                      ↓
    └──────────────────────────────────────────────────────┘
```

**Initial Stages:**
- Model generates mediocre synthetic examples
- Quality filter removes worst examples
- Remaining examples provide some rehearsal benefit

**Later Stages:**
- Improved model generates better synthetic examples
- More examples pass quality filter
- Higher quality rehearsal → better knowledge retention

**Risk: Generator Drift**
- If quality control fails, model may learn from bad examples
- Leads to distribution shift over tasks
- Mitigated by strict quality filtering

## 5. Implementation Details

### Synthesis Model Implementation

**Generation Strategy:**

**Option 1: Separate Input/Output Generation**
```python
def generate_example(model, task_description, temperature=1.0):
    # Step 1: Generate input
    input_prompt = f"Generate an input for: {task_description}\nInput:"
    generated_input = model.generate(
        input_prompt,
        max_length=128,
        temperature=temperature,
        top_p=0.95
    )

    # Step 2: Generate output given input
    output_prompt = f"Task: {task_description}\nInput: {generated_input}\nOutput:"
    generated_output = model.generate(
        output_prompt,
        max_length=64,
        temperature=temperature,
        top_p=0.95
    )

    return generated_input, generated_output
```

**Option 2: Joint Generation**
```python
def generate_example_joint(model, task_description):
    prompt = (
        f"Generate a complete training example for: {task_description}\n"
        f"Input: [generate]\n"
        f"Output: [generate]"
    )

    full_example = model.generate(prompt, max_length=256)

    # Parse into input and output
    input_part, output_part = parse_example(full_example)

    return input_part, output_part
```

**Option 3: In-Context Learning**
```python
def generate_with_examples(model, task_description, few_shot_examples):
    # Provide few-shot examples from task
    prompt = f"Task: {task_description}\n\n"

    for ex in few_shot_examples:
        prompt += f"Input: {ex['input']}\nOutput: {ex['output']}\n\n"

    prompt += "Generate a new example:\nInput:"

    generated_input = model.generate(prompt)
    generated_output = model.generate(f"{prompt} {generated_input}\nOutput:")

    return generated_input, generated_output
```

### Quality Filter Implementation

```python
class QualityFilter:
    def __init__(
        self,
        model,
        perplexity_threshold=100.0,
        consistency_threshold=0.5
    ):
        self.model = model
        self.ppl_threshold = perplexity_threshold
        self.cons_threshold = consistency_threshold

    @torch.no_grad()
    def compute_perplexity(self, text):
        """Compute perplexity of generated text."""
        tokens = self.tokenizer(text, return_tensors="pt")
        outputs = self.model(**tokens, labels=tokens.input_ids)
        loss = outputs.loss

        perplexity = torch.exp(loss).item()
        return perplexity

    @torch.no_grad()
    def compute_consistency(self, input_text, output_text):
        """Check if model agrees with generated label."""
        # For classification
        full_text = f"{input_text}\nOutput:"
        logits = self.model(full_text)

        # Get probability of generated output
        output_tokens = self.tokenizer(output_text)
        prob = self._get_sequence_probability(logits, output_tokens)

        return prob

    def filter_examples(self, examples):
        """Filter synthetic examples by quality."""
        filtered = []

        for inp, out in examples:
            # Compute quality metrics
            ppl = self.compute_perplexity(f"{inp} {out}")
            cons = self.compute_consistency(inp, out)

            # Apply thresholds
            if ppl < self.ppl_threshold and cons > self.cons_threshold:
                filtered.append({
                    'input': inp,
                    'output': out,
                    'confidence': cons,
                    'perplexity': ppl
                })

        return filtered
```

### Curriculum Scheduler Implementation

```python
class CurriculumScheduler:
    def __init__(
        self,
        initial_ratio=0.2,
        final_ratio=0.8,
        warmup_steps=1000,
        schedule_type="linear"
    ):
        self.initial_ratio = initial_ratio
        self.final_ratio = final_ratio
        self.warmup_steps = warmup_steps
        self.schedule_type = schedule_type
        self.current_step = 0

    def step(self):
        """Increment scheduler step."""
        self.current_step += 1

    def get_ratio(self):
        """Get current synthetic data ratio."""
        if self.current_step >= self.warmup_steps:
            return self.final_ratio

        progress = self.current_step / self.warmup_steps

        if self.schedule_type == "linear":
            ratio = self.initial_ratio + (self.final_ratio - self.initial_ratio) * progress

        elif self.schedule_type == "cosine":
            ratio = self.final_ratio - (self.final_ratio - self.initial_ratio) * (
                (1 + math.cos(math.pi * progress)) / 2
            )

        elif self.schedule_type == "exponential":
            ratio = self.final_ratio - (self.final_ratio - self.initial_ratio) * math.exp(-5 * progress)

        return ratio

    def create_mixed_batch(self, real_data, synthetic_pool, batch_size):
        """Create a batch mixing real and synthetic data."""
        ratio = self.get_ratio()

        n_synthetic = int(batch_size * ratio)
        n_real = batch_size - n_synthetic

        # Sample from each source
        real_batch = random.sample(real_data, n_real)
        synthetic_batch = random.sample(synthetic_pool, n_synthetic)

        # Combine and shuffle
        mixed_batch = real_batch + synthetic_batch
        random.shuffle(mixed_batch)

        return mixed_batch
```

### Full SSR Training Loop

```python
class SSRTrainer:
    def __init__(
        self,
        model,
        num_synthesis_examples=1000,
        synthesis_batch_size=8,
        perplexity_threshold=100.0,
        consistency_threshold=0.5,
        initial_synthetic_ratio=0.2,
        final_synthetic_ratio=0.8
    ):
        self.model = model
        self.num_synthesis = num_synthesis_examples
        self.synthesis_batch_size = synthesis_batch_size

        # Quality filter
        self.quality_filter = QualityFilter(
            model,
            perplexity_threshold,
            consistency_threshold
        )

        # Curriculum scheduler
        self.scheduler = CurriculumScheduler(
            initial_synthetic_ratio,
            final_synthetic_ratio
        )

        # Synthetic memory
        self.synthetic_memory = []

    def synthesize_task_data(self, task_description):
        """Generate synthetic examples for a task."""
        synthetic_examples = []

        while len(synthetic_examples) < self.num_synthesis:
            # Generate batch
            batch = []
            for _ in range(self.synthesis_batch_size):
                inp, out = self._generate_example(task_description)
                batch.append((inp, out))

            # Filter by quality
            filtered = self.quality_filter.filter_examples(batch)
            synthetic_examples.extend(filtered)

        return synthetic_examples[:self.num_synthesis]

    def train_task(
        self,
        task_data,
        task_description,
        num_epochs=3,
        learning_rate=5e-5
    ):
        """Train on a task with self-synthesized rehearsal."""
        optimizer = AdamW(self.model.parameters(), lr=learning_rate)

        self.model.train()

        for epoch in range(num_epochs):
            for real_batch in task_data:
                # Create mixed batch if we have synthetic data
                if self.synthetic_memory:
                    batch = self.scheduler.create_mixed_batch(
                        real_batch,
                        self.synthetic_memory,
                        len(real_batch)
                    )
                else:
                    batch = real_batch

                # Forward pass
                loss = self._compute_loss(batch)

                # Backward and update
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # Update curriculum
                self.scheduler.step()

        # After training, synthesize examples for this task
        new_synthetic = self.synthesize_task_data(task_description)
        self.synthetic_memory.extend(new_synthetic)

    def _generate_example(self, task_description):
        """Generate a single example (placeholder)."""
        # Implementation depends on model API
        pass

    def _compute_loss(self, batch):
        """Compute loss for a batch (placeholder)."""
        # Implementation depends on task type
        pass
```

### Memory Management

**Storage Requirements:**
```python
# Only store task descriptions (minimal)
task_descriptions = ["Sentiment classification", "QA", ...]  # ~1 KB

# Synthetic examples stored as text (not raw data)
# Can be regenerated on-demand
synthetic_cache_size = 1000 examples × 200 tokens × 2 bytes = 400 KB per task

# Compare to traditional rehearsal
real_data_storage = 1000 examples × full_encoding = 10 MB per task
```

**On-Demand Generation:**
```python
class OnDemandSynthesizer:
    """Generate synthetic data on-the-fly without storage."""

    def __init__(self, model, task_descriptions):
        self.model = model
        self.task_descriptions = task_descriptions
        self.quality_filter = QualityFilter(model)

    def get_rehearsal_batch(self, batch_size):
        """Generate synthetic batch on-demand."""
        # Sample task uniformly
        task_desc = random.choice(self.task_descriptions)

        # Generate examples
        examples = []
        while len(examples) < batch_size:
            inp, out = generate_example(self.model, task_desc)

            # Quick quality check
            if self._quick_quality_check(inp, out):
                examples.append((inp, out))

        return examples

    def _quick_quality_check(self, inp, out):
        """Lightweight quality check for efficiency."""
        # Simple heuristics without full model eval
        return len(inp.split()) > 3 and len(out.split()) > 1
```

## 6. Code Walkthrough

Let's walk through the implementation in `nexus/models/continual/self_synthesized_rehearsal.py`:

### SyntheticExample Container

```python
class SyntheticExample:
    """Container for a synthesized example."""

    def __init__(self, input_text, target_text, task_id, confidence=1.0):
        self.input_text = input_text
        self.target_text = target_text
        self.task_id = task_id
        self.confidence = confidence  # Quality score from filter
```

**Design Choice:**
- Store as text rather than embeddings (more flexible, human-readable)
- Include confidence score for weighted replay
- Track task_id for task-specific analysis

### SynthesisModel Class

```python
class SynthesisModel(NexusModule):
    """Model for synthesizing rehearsal examples."""

    def __init__(self, config):
        super().__init__(config)
        self.model = config.get("model")  # Base LLM
        self.max_length = config.get("max_length", 128)
        self.temperature = config.get("temperature", 1.0)
        self.top_k = config.get("top_k", 50)
        self.top_p = config.get("top_p", 0.95)
```

**Parameters:**
- `temperature`: Controls randomness (higher → more diverse, lower → more conservative)
- `top_k`: Sample from top-k tokens (controls diversity)
- `top_p`: Nucleus sampling threshold (quality vs. diversity trade-off)

### Synthesis Method

```python
@torch.no_grad()
def synthesize_examples(
    self,
    task_description,
    task_id,
    num_examples=100,
    batch_size=8
):
    """Generate synthetic examples for a task."""
    self.model.train(mode=False)  # Inference mode

    synthetic_examples = []
    num_batches = (num_examples + batch_size - 1) // batch_size

    for _ in range(num_batches):
        # Create synthesis prompts
        prompts = [
            f"Generate a training example for: {task_description}\nInput:"
        ] * min(batch_size, num_examples - len(synthetic_examples))

        # Generate inputs
        generated_inputs = self._generate_batch(prompts)

        # For each input, generate corresponding output
        for gen_input in generated_inputs:
            target_prompt = (
                f"Task: {task_description}\n"
                f"Input: {gen_input}\n"
                f"Output:"
            )
            generated_target = self._generate_batch([target_prompt])[0]

            synthetic_examples.append(
                SyntheticExample(
                    input_text=gen_input,
                    target_text=generated_target,
                    task_id=task_id,
                    confidence=1.0  # Updated by quality filter
                )
            )

    return synthetic_examples[:num_examples]
```

**Two-Stage Generation:**
1. Generate input given task description
2. Generate output given input and task description

**Why Not Joint Generation?**
- Separate stages provide better control
- Easier to apply quality checks
- More consistent with supervised learning format

### QualityFilter Class

```python
class QualityFilter(NexusModule):
    """Filters low-quality synthesized examples."""

    def __init__(self, config):
        super().__init__(config)
        self.model = config.get("model")
        self.perplexity_threshold = config.get("perplexity_threshold", 100.0)
        self.consistency_threshold = config.get("consistency_threshold", 0.5)

    @torch.no_grad()
    def filter_examples(self, examples):
        """Filter synthetic examples by quality."""
        self.model.train(mode=False)

        filtered = []

        for example in examples:
            # Compute quality metrics
            perplexity = self._compute_perplexity(
                example.input_text,
                example.target_text
            )

            consistency = self._compute_consistency(
                example.input_text,
                example.target_text
            )

            # Update confidence
            example.confidence = consistency

            # Filter by thresholds
            if (perplexity < self.perplexity_threshold and
                consistency > self.consistency_threshold):
                filtered.append(example)

        return filtered
```

**Two-Metric Filtering:**
1. **Perplexity**: Measures fluency/naturalness
2. **Consistency**: Measures model agreement with label

**Why Both?**
- Perplexity alone: May keep fluent but incorrect examples
- Consistency alone: May keep confident but unnatural examples
- Together: Ensures fluent AND correct examples

### SSRModel Main Class

```python
class SSRModel(NexusModule):
    """Self-Synthesized Rehearsal continual learning model."""

    def __init__(self, config):
        super().__init__(config)
        self.model = config.get("model")

        # Sub-modules
        self.synthesizer = SynthesisModel(config)
        self.quality_filter = QualityFilter(config)
        self.scheduler = CurriculumScheduler(
            initial_synthetic_ratio=config.get("initial_synthetic_ratio", 0.2),
            final_synthetic_ratio=config.get("final_synthetic_ratio", 0.8),
            warmup_steps=config.get("warmup_steps", 1000)
        )

        # Synthetic memory
        self.synthetic_memory = {}  # task_id -> List[SyntheticExample]

        # Configuration
        self.num_synthesis_examples = config.get("num_synthesis_examples", 1000)
        self.synthesis_batch_size = config.get("synthesis_batch_size", 8)
```

**Modular Design:**
- Each component (synthesis, filtering, scheduling) is separate
- Easy to swap different implementations
- Clear separation of concerns

### Training Method

```python
def train_task(
    self,
    data_loader,
    task_id,
    task_description,
    num_epochs=3,
    learning_rate=5e-5
):
    """Train on a task with self-synthesized rehearsal."""
    optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)

    # Get all previous task synthetic examples
    rehearsal_examples = self.get_rehearsal_examples()

    history = {"loss": []}
    self.model.train()

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        num_batches = 0

        for batch in data_loader:
            # Mix real and synthetic data
            if rehearsal_examples:
                batch = self.scheduler.create_mixed_batch(
                    batch,
                    rehearsal_examples,
                    task_id
                )

            # Standard training step
            optimizer.zero_grad()
            loss = self._compute_loss(batch)  # Task-specific loss
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            num_batches += 1
            self.scheduler.step()

        history["loss"].append(epoch_loss / num_batches)

    # Synthesize examples for current task
    self.synthesize_task_memory(task_id, task_description)

    return history
```

**Training Flow:**
1. Retrieve synthetic examples from previous tasks
2. For each batch, mix real and synthetic data
3. Train with mixed data
4. After training, synthesize for current task

**Key Design Decision:**
Synthesis happens **after** training on each task, ensuring generated examples capture learned knowledge.

### Usage Example

```python
# Initialize SSR model
config = {
    "model": llm,  # Pre-trained LLM (T5, GPT, etc.)
    "num_synthesis_examples": 1000,
    "synthesis_batch_size": 8,
    "perplexity_threshold": 100.0,
    "consistency_threshold": 0.5,
    "initial_synthetic_ratio": 0.2,
    "final_synthetic_ratio": 0.8,
    "warmup_steps": 1000
}

ssr_model = SSRModel(config)

# Train on Task 1: Sentiment Classification
task1_loader = ...  # DataLoader for sentiment data
ssr_model.train_task(
    task1_loader,
    task_id=0,
    task_description="Classify sentiment of movie reviews",
    num_epochs=3,
    learning_rate=5e-5
)

# Train on Task 2: Question Answering
# (will rehearse synthetic examples from Task 1)
task2_loader = ...
ssr_model.train_task(
    task2_loader,
    task_id=1,
    task_description="Answer questions based on context",
    num_epochs=3,
    learning_rate=5e-5
)

# Evaluate on Task 1 (should maintain performance)
evaluate(ssr_model.model, task1_loader)
```

## 7. Optimization Tricks

### Synthesis Optimization

**1. Batch Generation for Efficiency**
```python
def batch_generate(model, prompts, max_length=128):
    """Generate multiple examples in parallel."""
    # Pad prompts to same length
    tokenized = tokenizer(
        prompts,
        padding=True,
        return_tensors="pt"
    )

    # Generate in batch (much faster than sequential)
    outputs = model.generate(
        **tokenized,
        max_length=max_length,
        num_return_sequences=1,
        do_sample=True
    )

    return tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

**2. Cached Generation**
```python
class CachedSynthesizer:
    """Cache synthetic examples to avoid regeneration."""

    def __init__(self, cache_size=10000):
        self.cache = {}
        self.cache_size = cache_size

    def get_or_generate(self, task_description, num_examples):
        """Get from cache or generate."""
        if task_description in self.cache:
            cached = self.cache[task_description]
            if len(cached) >= num_examples:
                return random.sample(cached, num_examples)

        # Generate and cache
        new_examples = synthesize_examples(task_description, num_examples)
        self.cache[task_description] = new_examples

        return new_examples
```

**3. Diverse Sampling Strategies**
```python
def diverse_sampling_generation(model, task_description, num_examples):
    """Generate diverse examples using different sampling strategies."""
    examples = []

    # Strategy 1: High temperature (more diverse)
    examples.extend(
        generate_with_temperature(model, task_description, num_examples // 3, temp=1.5)
    )

    # Strategy 2: Nucleus sampling (balanced)
    examples.extend(
        generate_with_nucleus(model, task_description, num_examples // 3, top_p=0.95)
    )

    # Strategy 3: Top-k sampling (more focused)
    examples.extend(
        generate_with_topk(model, task_description, num_examples // 3, top_k=50)
    )

    return examples
```

### Quality Filtering Optimization

**1. Two-Stage Filtering**
```python
def two_stage_filtering(examples, model):
    """Fast heuristic filter followed by model-based filter."""

    # Stage 1: Fast heuristic filtering
    candidates = []
    for ex in examples:
        # Simple checks (very fast)
        if (len(ex.input_text.split()) > 5 and
            len(ex.target_text.split()) > 1 and
            not contains_repetition(ex.input_text)):
            candidates.append(ex)

    # Stage 2: Model-based filtering (slower, more accurate)
    filtered = []
    for ex in candidates:
        ppl = compute_perplexity(model, ex)
        cons = compute_consistency(model, ex)

        if ppl < threshold and cons > threshold:
            filtered.append(ex)

    return filtered
```

**2. Batch Perplexity Computation**
```python
@torch.no_grad()
def batch_compute_perplexity(model, examples, batch_size=32):
    """Compute perplexity for multiple examples efficiently."""
    perplexities = []

    for i in range(0, len(examples), batch_size):
        batch = examples[i:i+batch_size]

        # Tokenize batch
        texts = [f"{ex.input_text} {ex.target_text}" for ex in batch]
        tokens = tokenizer(texts, padding=True, return_tensors="pt")

        # Forward pass
        outputs = model(**tokens, labels=tokens.input_ids)

        # Compute perplexity per example
        loss_per_example = outputs.loss  # If using reduction='none'
        ppl_per_example = torch.exp(loss_per_example)

        perplexities.extend(ppl_per_example.tolist())

    return perplexities
```

**3. Adaptive Thresholds**
```python
def adaptive_threshold_filtering(examples, target_keep_ratio=0.7):
    """Adjust thresholds to keep desired ratio of examples."""

    # Compute all metrics
    metrics = [(compute_ppl(ex), compute_cons(ex)) for ex in examples]

    # Sort by quality (lower ppl, higher consistency)
    scored = [(ex, ppl, cons) for ex, (ppl, cons) in zip(examples, metrics)]
    scored.sort(key=lambda x: (-x[2], x[1]))  # Sort by consistency desc, ppl asc

    # Keep top examples
    num_keep = int(len(scored) * target_keep_ratio)
    filtered = [ex for ex, _, _ in scored[:num_keep]]

    return filtered
```

### Curriculum Scheduling Optimization

**1. Validation-Based Adaptation**
```python
class AdaptiveCurriculumScheduler:
    """Adjust synthetic ratio based on validation performance."""

    def __init__(self, initial_ratio=0.2):
        self.ratio = initial_ratio
        self.performance_history = []

    def update(self, new_task_val_loss, old_tasks_val_loss):
        """Adjust ratio based on validation performance."""

        if new_task_val_loss > threshold_high:
            # New task learning suffering → reduce synthetic ratio
            self.ratio = max(0.1, self.ratio - 0.1)

        elif old_tasks_val_loss > threshold_high:
            # Old task forgetting → increase synthetic ratio
            self.ratio = min(0.9, self.ratio + 0.1)

        self.performance_history.append({
            'ratio': self.ratio,
            'new_loss': new_task_val_loss,
            'old_loss': old_tasks_val_loss
        })
```

**2. Task-Specific Mixing Ratios**
```python
def task_specific_curriculum(task_similarities, base_ratio=0.5):
    """Higher rehearsal for dissimilar tasks."""
    ratios = {}

    for task_id, similarity in task_similarities.items():
        # More dissimilar → higher synthetic ratio (more protection)
        ratios[task_id] = base_ratio + (1 - similarity) * 0.3

    return ratios
```

**3. Confidence-Weighted Sampling**
```python
def confidence_weighted_sampling(synthetic_pool, num_samples):
    """Sample synthetic examples weighted by confidence."""

    # Extract confidence scores
    confidences = [ex.confidence for ex in synthetic_pool]

    # Normalize to probabilities
    probs = np.array(confidences) / sum(confidences)

    # Sample weighted by confidence
    indices = np.random.choice(
        len(synthetic_pool),
        size=num_samples,
        replace=False,
        p=probs
    )

    return [synthetic_pool[i] for i in indices]
```

### Memory Optimization

**1. On-Demand Generation (Zero Storage)**
```python
class ZeroMemorySSR:
    """Generate synthetic data on-the-fly, store nothing."""

    def __init__(self, model, task_descriptions):
        self.model = model
        self.task_descriptions = task_descriptions

    def get_rehearsal_batch(self, batch_size):
        """Generate fresh synthetic batch."""
        task_id = random.randint(0, len(self.task_descriptions) - 1)
        task_desc = self.task_descriptions[task_id]

        examples = []
        while len(examples) < batch_size:
            ex = generate_example(self.model, task_desc)
            if quick_quality_check(ex):
                examples.append(ex)

        return examples
```

**2. Compressed Storage**
```python
import json
import gzip

def save_synthetic_compressed(examples, filepath):
    """Save synthetic examples with compression."""
    data = [
        {
            'input': ex.input_text,
            'output': ex.target_text,
            'confidence': ex.confidence
        }
        for ex in examples
    ]

    with gzip.open(filepath, 'wt', encoding='utf-8') as f:
        json.dump(data, f)

def load_synthetic_compressed(filepath):
    """Load compressed synthetic examples."""
    with gzip.open(filepath, 'rt', encoding='utf-8') as f:
        data = json.load(f)

    return [SyntheticExample(**item) for item in data]
```

**3. Example Deduplication**
```python
def deduplicate_synthetic(examples, similarity_threshold=0.9):
    """Remove near-duplicate synthetic examples."""
    unique_examples = []
    seen_embeddings = []

    for ex in examples:
        # Compute embedding
        emb = compute_embedding(ex.input_text)

        # Check similarity with existing examples
        is_duplicate = False
        for seen_emb in seen_embeddings:
            if cosine_similarity(emb, seen_emb) > similarity_threshold:
                is_duplicate = True
                break

        if not is_duplicate:
            unique_examples.append(ex)
            seen_embeddings.append(emb)

    return unique_examples
```

## 8. Experiments & Results

### Text Classification Benchmarks

**Setup:**
- Tasks: 5 text classification tasks (sentiment, topic, intent, emotion, spam detection)
- Model: T5-base (220M parameters)
- Synthesis: 1000 examples per task
- Training: 3 epochs per task, lr=5e-5

**Results:**

| Method | Task 5 Avg Acc | Task 1 Final Acc | Forgetting | Memory |
|--------|----------------|------------------|------------|---------|
| Fine-tuning | 67.2% | 42.1% | 43.7% | 0 MB |
| Experience Replay (500) | 81.3% | 76.4% | 18.2% | 50 MB |
| EWC (λ=5000) | 74.5% | 68.3% | 25.6% | 2 MB |
| SSR (1000 synth) | 79.8% | 74.1% | 19.4% | 0.5 MB |
| SSR + Quality Filter | 82.1% | 77.8% | 16.7% | 0.5 MB |

**Key Observations:**
- SSR approaches replay performance without storing real data
- Quality filtering significantly improves results
- 10× memory reduction compared to traditional replay
- Better than EWC, approaching replay methods

### Question Answering Tasks

**Setup:**
- Tasks: SQuAD, NewsQA, NaturalQuestions, TriviaQA, HotpotQA
- Model: T5-large (770M parameters)
- Synthesis: 2000 QA pairs per task
- Evaluation: Exact Match (EM) and F1 score

**Results (F1 Score):**

| Method | Avg F1 | Task 1 Final | Task 5 Current | Forgetting |
|--------|--------|--------------|----------------|------------|
| Fine-tuning | 58.3 | 41.2 | 72.1 | 38.5 |
| Replay (1000) | 71.4 | 68.9 | 74.3 | 11.2 |
| SSR (2000) | 69.7 | 66.5 | 73.8 | 13.8 |
| SSR + Curriculum | 72.1 | 69.3 | 74.5 | 10.4 |

**Key Observations:**
- Curriculum scheduling crucial for QA tasks
- SSR performs well on generation-heavy tasks
- Slightly behind replay but no data storage
- Synthetic QA pairs effectively capture task knowledge

### Summarization Tasks

**Setup:**
- Tasks: CNN/DM, XSum, Reddit TIFU, arXiv, PubMed
- Model: T5-base fine-tuned
- Synthesis: 1500 summaries per task
- Evaluation: ROUGE scores

**Results (ROUGE-L):**

| Method | Avg ROUGE-L | Backward Transfer | Forward Transfer |
|--------|-------------|-------------------|------------------|
| Fine-tuning | 28.4 | -15.3 | +2.1 |
| Replay (500) | 35.7 | -4.2 | +3.4 |
| SSR (1500) | 34.2 | -5.8 | +3.1 |
| SSR High Temp | 36.1 | -4.1 | +3.6 |

**Key Observations:**
- Higher temperature generation improves diversity
- SSR works well for open-ended generation tasks
- Forward transfer preserved (positive)
- Competitive with replay methods

### Ablation Studies

**Effect of Number of Synthetic Examples:**

| Synth Examples | Avg Accuracy | Forgetting | Synthesis Time |
|----------------|--------------|------------|----------------|
| 100 | 72.3% | 24.1% | 30s |
| 500 | 77.8% | 18.7% | 2.5min |
| 1000 | 79.8% | 16.7% | 5min |
| 2000 | 80.5% | 15.9% | 10min |
| 5000 | 80.7% | 15.8% | 25min |

**Optimal Range:** 1000-2000 examples provide best accuracy/efficiency trade-off.

**Effect of Quality Filtering:**

| Filtering | Avg Accuracy | Examples Kept | Quality |
|-----------|--------------|---------------|---------|
| No Filtering | 75.2% | 100% | Low |
| Perplexity Only | 77.3% | 82% | Medium |
| Consistency Only | 78.1% | 75% | Medium |
| Both (SSR) | 79.8% | 68% | High |
| Adaptive Threshold | 80.4% | 70% | High |

**Key Insight:** Both perplexity and consistency filtering are necessary.

**Effect of Curriculum Schedule:**

| Schedule | Avg Accuracy | Task 1 Final | Task 5 Final |
|----------|--------------|--------------|--------------|
| No Curriculum (α=0.5) | 76.4% | 71.2% | 82.1% |
| Linear (0.2→0.8) | 79.8% | 74.1% | 83.5% |
| Cosine | 79.5% | 73.8% | 83.7% |
| Exponential | 78.9% | 73.1% | 84.2% |
| Adaptive | 80.6% | 75.3% | 83.9% |

**Best:** Adaptive curriculum based on validation performance.

### Comparison with Continual Learning Methods

**Text Classification (5 Tasks):**

| Method | Avg Acc | Forgetting | Memory | Compute |
|--------|---------|------------|--------|---------|
| Fine-tuning | 67.2% | 43.7% | 0 | 1× |
| L2 Regularization | 70.1% | 38.2% | 0 | 1× |
| EWC | 74.5% | 25.6% | 2 MB | 1.2× |
| SI (Synaptic Intelligence) | 75.8% | 23.4% | 2 MB | 1.2× |
| ER (Experience Replay) | 81.3% | 18.2% | 50 MB | 1× |
| GEM | 82.7% | 16.8% | 50 MB | 1.5× |
| SSR (Ours) | 79.8% | 19.4% | 0.5 MB | 1.3× |
| SSR + EWC | 81.9% | 16.2% | 2.5 MB | 1.4× |

**Key Takeaways:**
- SSR provides best memory-accuracy trade-off
- Can be combined with EWC for further improvements
- Competitive with replay while being privacy-preserving
- Slight compute overhead for synthesis

### Generator Quality Analysis

**Distribution Shift Over Tasks:**

| Task | Perplexity (Real) | Perplexity (Synth) | KL Divergence |
|------|-------------------|--------------------|---------------|
| Task 1 | 45.2 | 48.7 | 0.08 |
| Task 3 | 47.1 | 52.3 | 0.12 |
| Task 5 | 46.8 | 54.1 | 0.15 |

**Observation:** Slight generator drift over tasks, but still acceptable.

**Human Evaluation of Synthetic Quality:**

| Metric | Score (1-5) | Agreement |
|--------|-------------|-----------|
| Fluency | 4.2 | 0.82 |
| Coherence | 4.0 | 0.78 |
| Task Relevance | 4.3 | 0.85 |
| Label Correctness | 3.9 | 0.74 |

**Conclusion:** Synthetic examples are generally high quality and task-relevant.

## 9. Common Pitfalls

### 1. Generator Drift

**Problem:**
Over multiple tasks, synthetic data quality degrades:
```python
# Task 1: High-quality synthesis
"The movie was excellent" → Positive ✓

# Task 5: Degraded synthesis
"Movie the was excellent great" → Positive ✗
```

**Causes:**
- Model learns from its own errors
- Accumulating biases over tasks
- Distribution shift from real data

**Solutions:**
```python
# Periodically refresh with real data
if task_id % 3 == 0:
    fine_tune_on_real_data(model, combined_real_data)

# Monitor generation quality
if avg_perplexity > threshold:
    warn("Generator drift detected!")
    reduce_synthetic_ratio()

# Use stricter quality filtering for later tasks
threshold = base_threshold * (1 + 0.1 * task_id)
```

### 2. Synthesis Computational Cost

**Problem:**
Generating thousands of examples is expensive:
```python
# Generate 1000 examples × 5 tasks = 5000 generations
# At 2s per generation = 10,000 seconds ≈ 3 hours
```

**Solutions:**
```python
# Batch generation
generated = model.generate_batch(prompts, batch_size=32)  # Much faster

# Cache and reuse
cache_synthetic_examples(task_id, generated_examples)

# Generate fewer examples
num_examples = max(100, dataset_size // 10)  # Adaptive to task size

# Generate on-the-fly during training
for batch in train_loader:
    synthetic_batch = generate_on_demand(num=batch_size // 2)
    mixed_batch = combine(batch, synthetic_batch)
```

### 3. Quality Control Failures

**Problem:**
Low-quality synthetic examples hurt performance:
```python
# Bad synthesis examples
"dog cat tree house" → "positive"  # Nonsensical
"" → "negative"  # Empty input
```

**Solutions:**
```python
# Multi-stage filtering
def robust_filtering(examples):
    # Stage 1: Basic heuristics
    examples = [ex for ex in examples if len(ex.input_text.split()) > 5]

    # Stage 2: Perplexity
    examples = [ex for ex in examples if perplexity(ex) < 100]

    # Stage 3: Consistency
    examples = [ex for ex in examples if consistency(ex) > 0.5]

    # Stage 4: Human-in-the-loop (sample)
    if len(examples) > 1000:
        sample = random.sample(examples, 100)
        if human_approval_rate(sample) < 0.8:
            raise QualityWarning("Low quality synthetic data")

    return examples
```

### 4. Task Description Quality

**Problem:**
Poor task descriptions lead to poor synthesis:
```python
# Bad description
"Task 1"  # Too vague

# Good description
"Classify movie reviews as positive, negative, or neutral based on sentiment"
```

**Solutions:**
```python
# Use detailed, specific descriptions
task_descriptions = {
    'sentiment': (
        "Classify the sentiment of the given text as positive, negative, or neutral. "
        "Focus on the overall emotional tone and opinion expressed."
    ),
    'qa': (
        "Given a context passage, answer the question using information from the passage. "
        "The answer should be a span of text from the context."
    )
}

# Include few-shot examples in description
task_desc_with_examples = f"""
Task: {base_description}

Example 1:
Input: {ex1_input}
Output: {ex1_output}

Example 2:
Input: {ex2_input}
Output: {ex2_output}

Now generate similar examples.
"""
```

### 5. Curriculum Scheduling Issues

**Problem:**
Fixed curriculum doesn't adapt to task difficulty:
```python
# Easy task: Can use more synthetic data early
# Hard task: Needs more real data throughout
```

**Solutions:**
```python
# Task-difficulty-aware curriculum
class AdaptiveCurriculum:
    def get_ratio(self, task_difficulty, current_step):
        if task_difficulty == "easy":
            return 0.5 + 0.3 * (current_step / total_steps)
        elif task_difficulty == "hard":
            return 0.2 + 0.3 * (current_step / total_steps)

# Validation-driven adaptation
if new_task_val_loss > threshold:
    # Struggling on new task → reduce synthetic ratio
    synthetic_ratio -= 0.1

if old_tasks_val_loss > threshold:
    # Forgetting old tasks → increase synthetic ratio
    synthetic_ratio += 0.1
```

### 6. Memory Leakage in Generation

**Problem:**
Generation process accumulates memory:
```python
for i in range(10000):  # Generate many examples
    ex = model.generate(...)  # Memory not freed
    examples.append(ex)
    # Memory usage grows!
```

**Solutions:**
```python
# Use context managers
with torch.no_grad():
    for batch in generation_batches:
        examples.extend(model.generate(batch))

# Clear cache periodically
if i % 100 == 0:
    torch.cuda.empty_cache()
    gc.collect()

# Generate in batches with cleanup
def generate_with_cleanup(model, num_examples, batch_size=32):
    all_examples = []
    for i in range(0, num_examples, batch_size):
        batch_examples = model.generate_batch(batch_size)
        all_examples.extend(batch_examples)

        # Cleanup
        del batch_examples
        if i % 500 == 0:
            torch.cuda.empty_cache()

    return all_examples
```

### 7. Label Imbalance in Synthesis

**Problem:**
Model may generate imbalanced synthetic data:
```python
# Synthetic distribution
Positive: 80%
Negative: 15%
Neutral: 5%

# Real distribution
Positive: 40%
Negative: 40%
Neutral: 20%
```

**Solutions:**
```python
# Class-conditioned generation
def balanced_synthesis(model, task_desc, num_per_class):
    examples = []
    for class_label in classes:
        class_prompt = f"{task_desc}\nGenerate examples with label: {class_label}"
        class_examples = generate(model, class_prompt, num_per_class)
        examples.extend(class_examples)
    return examples

# Rejection sampling to match distribution
def match_distribution(synthetic_examples, target_distribution):
    balanced = []
    for label, target_ratio in target_distribution.items():
        label_examples = [ex for ex in synthetic_examples if ex.label == label]
        num_needed = int(total_examples * target_ratio)
        balanced.extend(random.sample(label_examples, num_needed))
    return balanced
```

### 8. Not Suitable for All Task Types

**Problem:**
SSR doesn't work well for certain tasks:
```python
# Works well: Text classification, QA, summarization
# Struggles: Fine-grained visual recognition, structured prediction
```

**Solutions:**
```python
# Task-type selection
def should_use_ssr(task_type):
    generative_tasks = ['text_classification', 'qa', 'summarization', 'translation']
    return task_type in generative_tasks

# Hybrid approach
if task_supports_generation:
    use_ssr()
else:
    use_traditional_rehearsal() or use_ewc()

# Multi-modal adaptation
for vision_language_tasks:
    synthesize_text_descriptions()  # Can generate text
    store_visual_features()  # Hard to generate images
```

### 9. Privacy Concerns Despite No Data Storage

**Problem:**
Model may memorize and regenerate training data:
```python
# Synthetic "example" is actually memorized training data
generated = "Exact copy of private email from training set"
```

**Solutions:**
```python
# Check for memorization
def detect_memorization(synthetic_example, training_data):
    for train_ex in training_data:
        if high_similarity(synthetic_example, train_ex):
            return True  # Reject memorized example
    return False

# Differential privacy in generation
def dp_generate(model, prompt, epsilon=1.0):
    # Add noise to generation process
    logits = model(prompt)
    noisy_logits = logits + noise(epsilon)
    return sample(noisy_logits)

# Temperature control
high_temperature = 1.5  # Reduces memorization, increases diversity
```

### 10. Forgetting Accumulation

**Problem:**
Performance still degrades over many tasks:
```python
# Performance over tasks
[95%, 93%, 89%, 84%, 78%, 71%, ...]  # Gradual decline
```

**Solutions:**
```python
# Hybrid SSR + Rehearsal
if critical_tasks:
    store_small_real_buffer(size=100)  # Supplement synthetic data
    combine_real_and_synthetic()

# Periodic consolidation
if task_id % 5 == 0:
    consolidate_across_all_tasks()
    regenerate_all_synthetic_data()

# Increase synthetic ratio over time
synthetic_ratio = min(0.95, 0.2 + 0.1 * task_id)
```

## 10. References

### Original Paper

**Self-Synthesized Rehearsal:**
- Sun, Z., et al. (2024). "Self-Synthesized Rehearsal: Reshaping the Learning Curriculum for Continual Learning with Large Language Models." *Annual Meeting of the Association for Computational Linguistics (ACL)*.
  - Introduces self-synthesis for LLM continual learning
  - Demonstrates quality filtering and curriculum reshaping
  - Shows competitive performance with traditional rehearsal

### Generative Replay Foundations

**Early Generative Replay:**
- Shin, H., et al. (2017). "Continual learning with deep generative replay." *Advances in Neural Information Processing Systems (NeurIPS)*.
  - Original generative replay with separate generator
  - Proves replay from synthetic data is effective

**Pseudo-Rehearsal:**
- Robins, A. (1995). "Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without Catastrophic Forgetting." *Connection Science*, 7(2), 123-146.
  - Classical pseudo-rehearsal approach
  - Theoretical foundations for synthetic replay

**DGR (Deep Generative Replay):**
- Kemker, R., & Kanan, C. (2018). "FearNet: Brain-inspired model for incremental learning." *International Conference on Learning Representations (ICLR)*.
  - Brain-inspired generative replay
  - Demonstrates effectiveness on vision tasks

### LLM-Specific Continual Learning

**Continual Pre-training:**
- Jang, J., et al. (2022). "Continual learning with large language models: A review." *arXiv preprint arXiv:2202.08377*.
  - Survey of continual learning for LLMs
  - Discusses challenges specific to large models

**Generative Replay for NLP:**
- Sun, F., et al. (2020). "LAMOL: Language modeling for lifelong language learning." *International Conference on Learning Representations (ICLR)*.
  - Uses LM objective for continual learning
  - Generates pseudo-samples for replay

**Distillation-Based Methods:**
- Cao, Y., et al. (2020). "Continual learning for text classification with information disentanglement based regularization." *North American Chapter of the Association for Computational Linguistics (NAACL)*.
  - Combines distillation with continual learning

### Quality Control and Filtering

**Data Quality in Generation:**
- Welleck, S., et al. (2019). "Neural text generation with unlikelihood training." *International Conference on Learning Representations (ICLR)*.
  - Improving generation quality
  - Relevant to filtering low-quality outputs

**Perplexity-Based Filtering:**
- Holtzman, A., et al. (2020). "The curious case of neural text degeneration." *International Conference on Learning Representations (ICLR)*.
  - Discusses generation quality metrics
  - Nucleus sampling for better generation

**Self-Consistency:**
- Wang, X., et al. (2022). "Self-consistency improves chain of thought reasoning in language models." *arXiv preprint arXiv:2203.11171*.
  - Using model self-consistency for validation

### Curriculum Learning

**Curriculum Learning Foundations:**
- Bengio, Y., et al. (2009). "Curriculum learning." *International Conference on Machine Learning (ICML)*.
  - Original curriculum learning paper
  - Progressive difficulty scheduling

**Dynamic Curriculum:**
- Graves, A., et al. (2017). "Automated curriculum learning for neural networks." *International Conference on Machine Learning (ICML)*.
  - Adaptive curriculum based on progress

### Related Continual Learning Methods

**Regularization-Based:**
- Kirkpatrick, J., et al. (2017). "Overcoming catastrophic forgetting in neural networks." *PNAS*.
  - EWC: Can be combined with SSR

**Rehearsal-Based:**
- Chaudhry, A., et al. (2019). "On tiny episodic memories in continual learning." *arXiv preprint arXiv:1902.10486*.
  - Experience Replay baseline

**Parameter Isolation:**
- Serra, J., et al. (2018). "Overcoming catastrophic forgetting with hard attention to the task." *International Conference on Machine Learning (ICML)*.
  - PackNet and HAT methods

### Privacy and Memorization

**Privacy in Machine Learning:**
- Carlini, N., et al. (2021). "Extracting training data from large language models." *USENIX Security*.
  - Memorization in language models
  - Relevant to privacy concerns in SSR

**Differential Privacy:**
- Abadi, M., et al. (2016). "Deep learning with differential privacy." *ACM SIGSAC Conference on Computer and Communications Security*.
  - DP for training neural networks

### Implementations and Tools

**Continual Learning Libraries:**
- Avalanche: https://github.com/ContinualAI/avalanche
  - Comprehensive continual learning framework
- Mammoth: https://github.com/aimagelab/mammoth
  - PyTorch continual learning toolkit

**LLM Generation Libraries:**
- Hugging Face Transformers: https://github.com/huggingface/transformers
  - Text generation with T5, GPT, etc.
- PEFT: https://github.com/huggingface/peft
  - Parameter-efficient fine-tuning for LLMs

### Benchmarks

**Continual Learning Benchmarks:**
- Lomonaco, V., et al. (2021). "Avalanche: An end-to-end library for continual learning." *CVPR Workshop*.
- Hemati, H., et al. (2023). "Continual Learning of Natural Language Processing Tasks: A Survey." *arXiv preprint*.

**LLM Evaluation:**
- Wang, A., et al. (2019). "GLUE: A multi-task benchmark and analysis platform for natural language understanding." *ICLR*.
- Wang, A., et al. (2019). "SuperGLUE: A stickier benchmark for general-purpose language understanding systems." *NeurIPS*.
