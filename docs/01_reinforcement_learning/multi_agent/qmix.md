# QMIX: Monotonic Value Function Factorisation

## 1. Overview

QMIX is a value-based multi-agent reinforcement learning algorithm that factorizes a joint action-value function into per-agent value functions using a monotonic mixing network. The monotonicity constraint ensures that the greedy action selection is consistent between the joint and individual Q-functions, enabling decentralized execution while maintaining expressiveness.

**Paper**: "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning" (Rashid et al., ICML 2018)

**Key Innovation**: State-dependent mixing network with hypernetworks that generate non-negative weights, guaranteeing monotonicity while enabling more expressive factorization than linear methods.

**Use Cases**:
- StarCraft micromanagement
- Network packet routing
- Traffic signal control
- Warehouse robotics
- Any discrete cooperative multi-agent task

## 2. Theory and Background

### 2.1 The Factorization Problem

In cooperative multi-agent RL, the team's joint Q-function Q_tot(s, a1, ..., an) captures the value of all agents taking actions together. The challenge is to factorize this into individual agent Q-functions Q_i(τ_i, a_i) where τ_i is agent i's action-observation history.

**Goal**: Enable decentralized execution where each agent can independently select:
```
a_i* = argmax_a_i Q_i(τ_i, a_i)
```

And have this correspond to the optimal joint action:
```
(a_1*, ..., a_n*) = argmax_(a_1,...,a_n) Q_tot(s, a_1, ..., a_n)
```

### 2.2 Individual-Global-Max (IGM) Condition

For decentralized execution to be optimal, we need:

```
argmax_a Q_tot(s, a) = (argmax_a1 Q_1(τ_1, a_1), ..., argmax_an Q_n(τ_n, a_n))
```

**QMIX's Solution**: Enforce monotonicity:
```
∂Q_tot / ∂Q_i ≥ 0  for all i
```

If Q_tot is monotonically increasing in each Q_i, then maximizing each Q_i independently also maximizes Q_tot.

### 2.3 Mixing Network Architecture

QMIX uses a mixing network f with state-dependent weights generated by hypernetworks:

```
Q_tot(s, a) = f(Q_1(τ_1, a_1), ..., Q_n(τ_n, a_n); s)
```

The mixing function is a feedforward network with:
- **Hidden layer**: h = ELU(|W_1(s)| · q + b_1(s))
- **Output layer**: Q_tot = |W_2(s)| · h + b_2(s)
- **Weights**: W_1, W_2 generated by hypernetworks, constrained to be non-negative via absolute value

The non-negative weights guarantee monotonicity: ∂Q_tot / ∂Q_i = W_2 · ∂h/∂Q_i ≥ 0

## 3. Mathematical Formulation

### Agent Q-Network

Each agent i has a Q-network that takes its observation-action history τ_i:

```
Q_i(τ_i, a_i; θ_i): H_i → R^|A_i|
```

Where H_i is the space of possible histories. Often implemented with RNNs (GRU):
```
h_i^t = GRU(h_i^{t-1}, [o_i^t, a_i^{t-1}])
Q_i(τ_i, ·) = MLP(h_i^t)
```

### Hypernetworks

Generate mixing weights from global state s:

```
W_1(s) = |hyperw1(s)|  # shape: [n_agents, mixing_dim]
b_1(s) = hyperb1(s)    # shape: [mixing_dim]
W_2(s) = |hyperw2(s)|  # shape: [mixing_dim, 1]
b_2(s) = hyperb2(s)    # shape: [1]
```

The absolute value ensures non-negativity, guaranteeing monotonicity.

### Complete Forward Pass

```
# 1. Compute individual Q-values
q_i = Q_i(τ_i, a_i)  for i = 1, ..., n

# 2. Stack into vector
q = [q_1, ..., q_n]^T  # shape: [n_agents, 1]

# 3. Generate mixing weights
W_1 = |hyperw1(s)|  # [n_agents, mixing_dim]
b_1 = hyperb1(s)    # [mixing_dim]
W_2 = |hyperw2(s)|  # [mixing_dim, 1]
b_2 = hyperb2(s)    # [1]

# 4. Mix values
h = ELU(W_1^T q + b_1)  # [mixing_dim]
Q_tot = W_2^T h + b_2   # scalar
```

### Loss Function

QMIX optimizes the standard DQN loss on the joint Q-function:

```
L(θ) = E[(Q_tot(s, a; θ) - y)^2]
```

Where the target is:
```
y = r + γ max_a' Q_tot(s', a'; θ^-)
```

With target network parameters θ^- updated via soft updates or periodic copies.

## 4. Intuitive Explanation

Think of QMIX as a team sport where:

1. **Individual Players** (agents): Each has their own skill assessment (Q-function) based on what they see
2. **Team Coach** (mixing network): Combines individual assessments into a team strategy
3. **Monotonicity**: If any player improves their play, the team's overall performance can't get worse
4. **State-Dependent**: The coach adjusts how to weigh each player's contribution based on the game situation

### Why Monotonicity Helps

Without monotonicity, maximizing individual Q-values might not maximize team value. Example:

```
Q_1(s, a_1=left) = 10,   Q_2(s, a_2=left) = 10
Q_1(s, a_1=right) = 5,   Q_2(s, a_2=right) = 5

Non-monotonic mixing:
Q_tot(s, left, left) = 5    # Both go left (bad)
Q_tot(s, left, right) = 20  # One left, one right (good)
Q_tot(s, right, left) = 20  # One right, one left (good)
Q_tot(s, right, right) = 5  # Both go right (bad)
```

Each agent would choose left (higher individual Q), but the optimal joint action is mixed. Monotonic mixing can't represent this!

**QMIX's Limitation**: Can only represent tasks where individual greedy actions align with team optimality.

## 5. Implementation Details

### Network Architecture

```python
# Agent Q-Network (per agent)
obs -> [Linear(64) + ReLU] x2 -> Linear(n_actions)

# Mixing Network Hypernetworks
state -> [Linear(mixing_dim) + ReLU + Linear(n_agents * mixing_dim)] -> abs() -> W1
state -> Linear(mixing_dim) -> b1
state -> [Linear(mixing_dim) + ReLU + Linear(mixing_dim)] -> abs() -> W2
state -> [Linear(mixing_dim) + ReLU + Linear(1)] -> b2

# Mixing Network
q = [Q1(τ1, a1), ..., Qn(τn, an)]
h = ELU(W1 @ q + b1)
Q_tot = W2 @ h + b2
```

### Hyperparameters

| Parameter | Typical Value | Description |
|-----------|---------------|-------------|
| gamma | 0.99 | Discount factor |
| tau | 0.005 | Target network soft update rate |
| epsilon | 1.0 → 0.05 | Epsilon-greedy exploration (decay) |
| epsilon_decay | 0.995 | Decay rate per episode |
| learning_rate | 3e-4 | Learning rate |
| hidden_dim | 64 | Agent network hidden size |
| mixing_dim | 32 | Mixing network embedding size |
| batch_size | 32 | Replay buffer batch size |
| buffer_size | 5000 | Replay buffer capacity (episodes) |

### Training Loop

```python
# Initialize networks
agent_networks = [AgentNetwork(obs_dim, n_actions) for _ in range(n_agents)]
mixing_network = MixingNetwork(n_agents, state_dim, mixing_dim)
target_agent_networks = deepcopy(agent_networks)
target_mixing_network = deepcopy(mixing_network)

# Training loop
for episode in range(num_episodes):
    # Collect episode
    episode_buffer = []
    for t in range(episode_length):
        # Each agent selects action (epsilon-greedy)
        actions = []
        for i, agent_net in enumerate(agent_networks):
            if random() < epsilon:
                action = random_action()
            else:
                q_values = agent_net(obs[i])
                action = argmax(q_values)
            actions.append(action)

        # Execute actions
        next_obs, reward, done, state = env.step(actions)
        episode_buffer.append((obs, actions, reward, next_obs, done, state))
        obs = next_obs

    # Add to replay buffer
    replay_buffer.append(episode_buffer)

    # Sample batch and train
    if len(replay_buffer) >= batch_size:
        batch = sample(replay_buffer, batch_size)

        # Compute current Q_tot
        q_values = [agent_net(batch['obs'][i]) for i in range(n_agents)]
        chosen_q = [q[batch['actions'][i]] for i, q in enumerate(q_values)]
        q_tot = mixing_network(chosen_q, batch['state'])

        # Compute target Q_tot
        with torch.no_grad():
            target_q_values = [target_net(batch['next_obs'][i]) for i in range(n_agents)]
            target_max_q = [max(q) for q in target_q_values]
            target_q_tot = target_mixing_network(target_max_q, batch['next_state'])
            targets = batch['reward'] + gamma * (1 - batch['done']) * target_q_tot

        # Update
        loss = mse_loss(q_tot, targets)
        optimizer.zero_grad()
        loss.backward()
        clip_grad_norm_(parameters, max_grad_norm=10.0)
        optimizer.step()

        # Soft update targets
        soft_update(agent_networks, target_agent_networks, tau)
        soft_update(mixing_network, target_mixing_network, tau)

    # Decay epsilon
    epsilon = max(epsilon_min, epsilon * epsilon_decay)
```

## 6. Code Walkthrough

### Core Components (from `/nexus/models/rl/qmix.py`)

#### 1. Agent Q-Network

```python
class QMIXNetwork(NexusModule):
    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int = 64):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, obs: torch.Tensor) -> torch.Tensor:
        return self.network(obs)  # [batch, action_dim]
```

Simple feedforward network. For partial observability, replace with GRU:
```python
self.gru = nn.GRUCell(obs_dim, hidden_dim)
self.fc = nn.Linear(hidden_dim, action_dim)
```

#### 2. Mixing Network with Hypernetworks

```python
class MixingNetwork(NexusModule):
    def __init__(self, num_agents: int, state_dim: int, mixing_dim: int = 32):
        super().__init__()
        self.num_agents = num_agents
        self.mixing_dim = mixing_dim

        # Hypernetwork for first layer weights
        self.hyper_w1 = nn.Sequential(
            nn.Linear(state_dim, mixing_dim),
            nn.ReLU(),
            nn.Linear(mixing_dim, num_agents * mixing_dim)
        )
        self.hyper_b1 = nn.Linear(state_dim, mixing_dim)

        # Hypernetwork for second layer
        self.hyper_w2 = nn.Sequential(
            nn.Linear(state_dim, mixing_dim),
            nn.ReLU(),
            nn.Linear(mixing_dim, mixing_dim)
        )
        self.hyper_b2 = nn.Sequential(
            nn.Linear(state_dim, mixing_dim),
            nn.ReLU(),
            nn.Linear(mixing_dim, 1)
        )

    def forward(self, agent_qs: torch.Tensor, state: torch.Tensor) -> torch.Tensor:
        batch_size = agent_qs.size(0)

        # Generate weights (enforced non-negative)
        w1 = torch.abs(self.hyper_w1(state))  # [batch, num_agents * mixing_dim]
        w1 = w1.view(batch_size, self.num_agents, self.mixing_dim)
        b1 = self.hyper_b1(state).view(batch_size, 1, self.mixing_dim)

        # First layer
        agent_qs = agent_qs.view(batch_size, 1, self.num_agents)
        hidden = F.elu(torch.bmm(agent_qs, w1) + b1)  # [batch, 1, mixing_dim]

        # Second layer
        w2 = torch.abs(self.hyper_w2(state))  # [batch, mixing_dim]
        w2 = w2.view(batch_size, self.mixing_dim, 1)
        b2 = self.hyper_b2(state).view(batch_size, 1, 1)

        q_tot = torch.bmm(hidden, w2) + b2  # [batch, 1, 1]
        return q_tot.squeeze(-1).squeeze(-1)  # [batch]
```

**Key Points**:
- `torch.abs()` enforces non-negativity for monotonicity
- Hypernetworks generate state-dependent weights
- Batch matrix multiplication (`bmm`) for efficient computation
- ELU activation for hidden layer (smooth and non-saturating)

#### 3. Action Selection

```python
def select_action(self, observations: List[torch.Tensor], training: bool = True) -> List[int]:
    actions = []
    with torch.no_grad():
        for i, (net, obs) in enumerate(zip(self.agent_networks, observations)):
            if training and np.random.random() < self.epsilon:
                action = np.random.randint(self.action_dim)
            else:
                q_values = net(obs.unsqueeze(0) if obs.dim() == 1 else obs)
                action = q_values.argmax(dim=-1).item()
            actions.append(action)
    return actions
```

Epsilon-greedy exploration per agent, fully decentralized.

#### 4. Update Step

```python
def update(self, batch: Dict[str, Any]) -> Dict[str, float]:
    # Compute current Q_tot
    agent_qs = self._get_agent_q_values(
        batch["observations"], batch["actions"], target=False
    )
    q_tot = self.mixing_network(agent_qs, batch["state"])

    # Compute target Q_tot
    with torch.no_grad():
        target_agent_qs = self._get_target_max_q_values(batch["next_observations"])
        target_q_tot = self.target_mixing_network(target_agent_qs, batch["next_state"])
        targets = batch["rewards"] + self.gamma * (1 - batch["dones"]) * target_q_tot

    # TD loss
    loss = F.mse_loss(q_tot, targets.detach())

    # Optimize
    self.optimizer.zero_grad()
    loss.backward()
    nn.utils.clip_grad_norm_(self.parameters(), 10.0)
    self.optimizer.step()

    # Soft update targets
    self._soft_update()

    # Decay epsilon
    self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    return {"loss": loss.item(), "q_tot": q_tot.mean().item(), "epsilon": self.epsilon}
```

Standard DQN update applied to the mixed Q-function.

## 7. Optimization Tricks

### 7.1 Training Stability

1. **Gradient Clipping**: Essential for hypernetworks (use 10.0)
2. **Soft Target Updates**: tau=0.005 prevents oscillations
3. **Reward Scaling**: Normalize rewards to [-1, 1] or standardize
4. **Episode-based Replay**: Store full episodes, sample uniformly

### 7.2 Exploration

1. **Epsilon Decay Schedule**:
   ```python
   epsilon = max(epsilon_min, epsilon * decay_rate)  # Per episode
   # OR
   epsilon = epsilon_min + (1 - epsilon_min) * exp(-step / decay_steps)  # Exponential
   ```

2. **Boltzmann Exploration**: Alternative to epsilon-greedy:
   ```python
   probs = softmax(q_values / temperature)
   action = categorical_sample(probs)
   ```

3. **UCB Exploration**: Add exploration bonus based on visit counts

### 7.3 Hypernetwork Tricks

1. **Initialize Biases**: Initialize b2 to produce reasonable initial Q-values
   ```python
   self.hyper_b2[-1].bias.data.fill_(initial_q_value)
   ```

2. **Weight Initialization**: Xavier/He initialization for hypernetworks
   ```python
   nn.init.xavier_uniform_(self.hyper_w1[0].weight)
   ```

3. **Separate Learning Rates**: Lower LR for hypernetworks vs agent networks
   ```python
   optimizer = Adam([
       {'params': agent_params, 'lr': 3e-4},
       {'params': mixing_params, 'lr': 1e-4}
   ])
   ```

### 7.4 Replay Buffer Enhancements

1. **Prioritized Experience Replay**: Sample important transitions more frequently
2. **Episode Trimming**: Store only last N steps to save memory
3. **Sequence Sampling**: Sample consecutive transitions for RNN training

## 8. Experimental Results

### 8.1 SMAC Benchmark

**Environment**: StarCraft Multi-Agent Challenge

| Scenario | QMIX Win Rate | Training Steps | Description |
|----------|---------------|----------------|-------------|
| 2s3z | 95% | 2M | 2 Stalkers, 3 Zealots vs 2 Stalkers, 3 Zealots |
| 3s5z | 90% | 5M | 3 Stalkers, 5 Zealots vs 3 Stalkers, 5 Zealots |
| 1c3s5z | 85% | 10M | 1 Colossi, 3 Stalkers, 5 Zealots vs 1 Colossi, 3 Stalkers, 5 Zealots |
| MMM | 80% | 10M | 1 Medivac, 2 Marauders, 7 Marines vs 1 Medivac, 2 Marauders, 7 Marines |
| corridor | 98% | 5M | 6 Zealots in corridor vs 24 Zerglings |

**Key Findings**:
- QMIX excels at unit micromanagement requiring tight coordination
- Performance degrades on very large team sizes (>15 agents)
- Struggles when optimal strategy requires non-monotonic cooperation

### 8.2 Ablation Studies

| Component | Performance Impact | Notes |
|-----------|-------------------|-------|
| Mixing network | +40% vs VDN | State-dependent mixing crucial |
| Monotonicity | +25% vs no constraint | IGM guarantee helps convergence |
| Hypernetworks | +15% vs fixed weights | Adaptation to state critical |
| Target networks | +20% vs no targets | Stability essential |
| Gradient clipping | +10% | Prevents divergence |

### 8.3 Comparison with Baselines

On SMAC benchmark (average win rate across 14 scenarios):

| Algorithm | Win Rate | Sample Efficiency |
|-----------|----------|-------------------|
| QMIX | 82% | 5M steps |
| VDN | 65% | 7M steps |
| IQL | 45% | 8M steps |
| COMA | 58% | 10M steps |
| QTRAN | 75% | 6M steps |

## 9. Common Pitfalls and Solutions

### 9.1 Monotonicity Violation

**Problem**: Removing `torch.abs()` or using other mixing schemes breaks IGM

**Solution**: Always enforce non-negative weights. If you need more expressiveness, use WQMIX or QPLEX instead.

### 9.2 State Representation

**Problem**: Poor state representation hurts mixing network

**Solutions**:
- Ensure state includes all relevant global information
- Use position/relation encodings for spatial tasks
- Normalize state features
- Consider learned state embeddings

### 9.3 Exploration vs Exploitation

**Problem**: Premature convergence to suboptimal policies

**Solutions**:
- Slower epsilon decay (0.995 → 0.999)
- Longer exploration phase (decay after N episodes)
- Use Boltzmann exploration with annealed temperature
- Add intrinsic motivation (counts, prediction error)

### 9.4 Credit Assignment

**Problem**: Agents don't learn individual contributions

**Solutions**:
- Use agent-specific features in observations
- Add agent IDs to observations
- Increase mixing network capacity
- Use auxiliary losses (e.g., predict own reward)

### 9.5 Training Instability

**Problem**: Q-values explode or training collapses

**Solutions**:
- Aggressive gradient clipping (5.0 or lower)
- Reward normalization/clipping
- Smaller learning rate for mixing network
- Double Q-learning (use online network for action selection, target for evaluation)

## 10. Extensions and Variants

### 10.1 QMIX with RNNs

Add recurrence for partial observability:
```python
class QMIXRNNNetwork(nn.Module):
    def __init__(self, obs_dim, action_dim, hidden_dim=64):
        super().__init__()
        self.gru = nn.GRUCell(obs_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, action_dim)
        self.hidden_state = None

    def forward(self, obs, hidden=None):
        if hidden is None:
            hidden = torch.zeros(obs.size(0), self.gru.hidden_size)
        hidden = self.gru(obs, hidden)
        q_values = self.fc(hidden)
        return q_values, hidden
```

### 10.2 QMIX with Attention

Replace mixing network with attention mechanism:
```python
class AttentionMixing(nn.Module):
    def forward(self, agent_qs, state):
        # Query from state
        query = self.query_net(state)
        # Keys/values from agent Q-values
        keys = self.key_net(agent_qs)
        values = agent_qs
        # Attention-weighted sum
        attention = softmax(query @ keys.T / sqrt(d_k))
        q_tot = attention @ values
        return q_tot
```

### 10.3 Distributional QMIX

Extend to distributional RL for better value estimation:
```python
class DistributionalQMIX(QMIX):
    def __init__(self, n_atoms=51, v_min=-10, v_max=10):
        # Each agent outputs distribution over returns
        self.agent_networks = [
            DistributionalQNetwork(obs_dim, action_dim, n_atoms)
            for _ in range(n_agents)
        ]
        # Mixing network operates on distributions
```

### 10.4 Off-Policy QMIX

Improve sample efficiency with importance sampling:
```python
# Compute importance weights
behavior_probs = ...
target_probs = ...
importance_weights = target_probs / behavior_probs

# Weighted TD loss
loss = (importance_weights * td_error).mean()
```

### 10.5 Curiosity-Driven QMIX

Add intrinsic rewards for exploration:
```python
# Prediction error as intrinsic reward
intrinsic_reward = ||predict(next_state) - embed(next_state)||^2
total_reward = extrinsic_reward + beta * intrinsic_reward
```

## 11. References

### Original Papers

1. **QMIX**: Rashid et al., "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning", ICML 2018 [arXiv:1803.11485](https://arxiv.org/abs/1803.11485)

2. **VDN**: Sunehag et al., "Value-Decomposition Networks For Cooperative Multi-Agent Learning", AAMAS 2018

### Extensions

3. **QTRAN**: Son et al., "QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning", ICML 2019

4. **WQMIX**: Rashid et al., "Weighted QMIX: Expanding Monotonic Value Function Factorisation", NeurIPS 2020

5. **QPLEX**: Wang et al., "QPLEX: Duplex Dueling Multi-Agent Q-Learning", ICLR 2021

### Benchmarks

6. **SMAC**: Samvelyan et al., "The StarCraft Multi-Agent Challenge", AAMAS 2019 [GitHub](https://github.com/oxwhirl/smac)

7. **PyMARL**: PyMARL Framework [GitHub](https://github.com/oxwhirl/pymarl)

### Surveys

8. Zhang et al., "Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms", 2021

9. Oroojlooy & Hajinezhad, "A Review of Cooperative Multi-Agent Deep Reinforcement Learning", Applied Intelligence, 2023
